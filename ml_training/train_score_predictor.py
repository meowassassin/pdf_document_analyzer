#!/usr/bin/env python3
"""
êµ¬ì¡°ì  ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import json
from pathlib import Path
import argparse

# ëª¨ë¸ ì •ì˜
class ScorePredictorModel(nn.Module):
    def __init__(self, input_dim=7):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()  # 0~1 ë²”ìœ„ ì¶œë ¥
        )

    def forward(self, x):
        return self.network(x).squeeze()

def load_data(csv_path):
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    df = pd.read_csv(csv_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} samples")
    print(f"ì ìˆ˜ ë¶„í¬:\n{df['score'].describe()}")
    return df

def extract_features(df):
    """íŠ¹ì§• ì¶”ì¶œ"""
    feature_cols = [
        'importance',
        'resonance',
        'is_header',
        'length',
        'relative_font_size',
        'indent_level',
        'word_count'
    ]

    X = df[feature_cols].values
    y = df['score'].values

    return X, y, feature_cols

def train_model(X_train, y_train, X_val, y_val, epochs=100):
    """ëª¨ë¸ í•™ìŠµ"""
    model = ScorePredictorModel(input_dim=X_train.shape[1])
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # í…ì„œ ë³€í™˜
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.FloatTensor(y_val)

    best_val_loss = float('inf')
    best_model_state = None

    print("\nğŸš€ í•™ìŠµ ì‹œì‘...")
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

        # ê²€ì¦
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor).item()
            val_mae = torch.abs(val_outputs - y_val_tensor).mean().item()

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}")

    # ìµœì  ëª¨ë¸ ë³µì›
    model.load_state_dict(best_model_state)
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ - ìµœì € ê²€ì¦ Loss: {best_val_loss:.4f}")

    return model, best_val_loss

def evaluate_model(model, X_test, y_test):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.FloatTensor(y_test)

    with torch.no_grad():
        outputs = model(X_test_tensor)
        mse = nn.MSELoss()(outputs, y_test_tensor).item()
        mae = torch.abs(outputs - y_test_tensor).mean().item()
        rmse = np.sqrt(mse)

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  MSE: {mse:.4f}")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")

    return mae

def save_model(model, scaler, feature_names, test_mae, output_dir):
    """ëª¨ë¸ ì €ì¥"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # TorchScript ë³€í™˜
    model.eval()
    example_input = torch.randn(1, len(feature_names))
    traced_model = torch.jit.trace(model, example_input)

    model_path = output_dir / 'score_predictor.pt'
    traced_model.save(str(model_path))
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_path}")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'feature_names': feature_names,
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_std': scaler.scale_.tolist(),
        'test_mae': float(test_mae),
        'input_dim': len(feature_names)
    }

    metadata_path = output_dir / 'score_predictor_metadata.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

def main():
    parser = argparse.ArgumentParser(description='êµ¬ì¡°ì  ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--data', type=str, required=True, help='í•™ìŠµ ë°ì´í„° CSV íŒŒì¼')
    parser.add_argument('--output', type=str, default='../core/models', help='ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜')
    args = parser.parse_args()

    # ë°ì´í„° ë¡œë“œ
    df = load_data(args.data)

    # íŠ¹ì§• ì¶”ì¶œ
    X, y, feature_names = extract_features(df)

    # ë°ì´í„° ë¶„í• 
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=42
    )

    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {len(X_train)} samples")
    print(f"  Val: {len(X_val)} samples")
    print(f"  Test: {len(X_test)} samples")

    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    # ëª¨ë¸ í•™ìŠµ
    model, val_loss = train_model(X_train, y_train, X_val, y_val, args.epochs)

    # ëª¨ë¸ í‰ê°€
    test_mae = evaluate_model(model, X_test, y_test)

    # ëª¨ë¸ ì €ì¥
    save_model(model, scaler, feature_names, test_mae, args.output)

    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

if __name__ == '__main__':
    main()
