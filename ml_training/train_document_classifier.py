#!/usr/bin/env python3
"""
ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ê¸° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np
import json
from pathlib import Path
import argparse

# ëª¨ë¸ ì •ì˜
class DocumentClassifierModel(nn.Module):
    def __init__(self, input_dim=6, hidden_dim1=64, hidden_dim2=32, hidden_dim3=16, num_classes=5):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim1),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim1, hidden_dim2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim2, hidden_dim3),
            nn.ReLU(),
            nn.Linear(hidden_dim3, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.network(x)

def load_data(csv_path):
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    df = pd.read_csv(csv_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} samples")
    print(f"ë¼ë²¨ ë¶„í¬:\n{df['label'].value_counts()}")
    return df

def extract_features(df):
    """íŠ¹ì§• ì¶”ì¶œ"""
    feature_cols = [
        'headers_ratio',
        'lists_ratio',
        'paragraphs_ratio',
        'avg_length',
        'avg_importance',
        'log_cell_count'
    ]

    X = df[feature_cols].values
    y = df['label'].values

    return X, y, feature_cols

def train_model(X_train, y_train, X_val, y_val, num_classes, epochs=100):
    """ëª¨ë¸ í•™ìŠµ"""
    model = DocumentClassifierModel(input_dim=X_train.shape[1], num_classes=num_classes)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # í…ì„œ ë³€í™˜
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.LongTensor(y_val)

    best_val_acc = 0
    best_model_state = None

    print("\nğŸš€ í•™ìŠµ ì‹œì‘...")
    for epoch in range(epochs):
        # í•™ìŠµ
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

        # ê²€ì¦
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_preds = val_outputs.argmax(dim=1)
            val_acc = (val_preds == y_val_tensor).float().mean().item()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}, Val Acc: {val_acc:.4f}")

    # ìµœì  ëª¨ë¸ ë³µì›
    model.load_state_dict(best_model_state)
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ - ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}")

    return model, best_val_acc

def evaluate_model(model, X_test, y_test, label_encoder):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)

    with torch.no_grad():
        outputs = model(X_test_tensor)
        preds = outputs.argmax(dim=1)
        test_acc = (preds == y_test_tensor).float().mean().item()

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")

    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    print("\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:")
    for i, class_name in enumerate(label_encoder.classes_):
        mask = y_test == i
        if mask.sum() > 0:
            class_acc = (preds[mask] == y_test_tensor[mask]).float().mean().item()
            print(f"  {class_name}: {class_acc:.4f} ({mask.sum()} samples)")

    return test_acc

def save_model(model, scaler, label_encoder, feature_names, test_acc, output_dir):
    """ëª¨ë¸ ì €ì¥"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # TorchScript ë³€í™˜
    model.eval()
    example_input = torch.randn(1, len(feature_names))
    traced_model = torch.jit.trace(model, example_input)

    model_path = output_dir / 'document_classifier.pt'
    traced_model.save(str(model_path))
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_path}")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'feature_names': feature_names,
        'label_mapping': {i: label for i, label in enumerate(label_encoder.classes_)},
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_std': scaler.scale_.tolist(),
        'test_accuracy': float(test_acc),
        'num_classes': len(label_encoder.classes_),
        'input_dim': len(feature_names)
    }

    metadata_path = output_dir / 'model_metadata.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

def main():
    parser = argparse.ArgumentParser(description='ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ê¸° í•™ìŠµ')
    parser.add_argument('--data', type=str, required=True, help='í•™ìŠµ ë°ì´í„° CSV íŒŒì¼')
    parser.add_argument('--output', type=str, default='../core/models', help='ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--epochs', type=int, default=100, help='í•™ìŠµ ì—í­ ìˆ˜')
    args = parser.parse_args()

    # ë°ì´í„° ë¡œë“œ
    df = load_data(args.data)

    # íŠ¹ì§• ì¶”ì¶œ
    X, y, feature_names = extract_features(df)

    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # ë°ì´í„° ë¶„í• 
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
    )

    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  Train: {len(X_train)} samples")
    print(f"  Val: {len(X_val)} samples")
    print(f"  Test: {len(X_test)} samples")

    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    # ëª¨ë¸ í•™ìŠµ
    model, val_acc = train_model(X_train, y_train, X_val, y_val,
                                   len(label_encoder.classes_), args.epochs)

    # ëª¨ë¸ í‰ê°€
    test_acc = evaluate_model(model, X_test, y_test, label_encoder)

    # ëª¨ë¸ ì €ì¥
    save_model(model, scaler, label_encoder, feature_names, test_acc, args.output)

    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")

if __name__ == '__main__':
    main()
